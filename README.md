
# IBGE Vector Search + RAG ğŸ§ 

Este projeto demonstra um pipeline completo de indexaÃ§Ã£o vetorial de notÃ­cias do IBGE e resposta a perguntas usando RAG (Retrieval-Augmented Generation) com um modelo open-source de linguagem.

---

## ğŸ“¦ InstalaÃ§Ã£o

1. Clone ou extraia o repositÃ³rio:

```
unzip ibge_vec_project.zip
cd ibge_vec
```

2. Instale as dependÃªncias (idealmente em um ambiente virtual):

```
pip install -r requirements.txt
```

---

## âš™ï¸ Estrutura do Projeto

```
ibge_vec/
â”œâ”€â”€ config.py            # ConfiguraÃ§Ãµes globais e logger
â”œâ”€â”€ db.py                # CriaÃ§Ã£o do banco vetorial com SQLite + sqlite-vec
â”œâ”€â”€ embed.py             # GeraÃ§Ã£o de embeddings com SentenceTransformer
â”œâ”€â”€ fetch_ibge.py        # Coleta e chunking das notÃ­cias da API do IBGE
â”œâ”€â”€ llm.py               # Carregamento do modelo de linguagem (ex: Mistral)
â”œâ”€â”€ metrics.py           # Registro de mÃ©tricas de uso e embeddings
â”œâ”€â”€ rag.py               # Pipeline de RAG: recuperaÃ§Ã£o + geraÃ§Ã£o
â”œâ”€â”€ search.py            # Busca vetorial simples
â”œâ”€â”€ main.py              # Pipeline de ingestÃ£o e indexaÃ§Ã£o
â”œâ”€â”€ run_rag.py           # Executa uma pergunta via RAG
â””â”€â”€ requirements.txt     # DependÃªncias do projeto
```

---

## ğŸš€ Como Usar

### 1. Indexar as notÃ­cias (com embeddings)
```
python main.py
```

Isso irÃ¡:
- Buscar as Ãºltimas 5 notÃ­cias do IBGE sobre "inflaÃ§Ã£o"
- Gerar embeddings com `all-MiniLM-L6-v2`
- Criar um banco `ibge_noticias_local.db`
- Registrar mÃ©tricas em `metricas.csv`

### 2. Fazer perguntas com RAG (modelo local)
```
python run_rag.py
```

VocÃª poderÃ¡ digitar perguntas como:
- `Qual foi o impacto da inflaÃ§Ã£o nos alimentos?`
- `O que o IBGE divulgou recentemente sobre preÃ§os?`

---

## ğŸ§ª MÃ©tricas

As mÃ©tricas de uso sÃ£o salvas em `metricas.csv`, incluindo:
- Tempo de geraÃ§Ã£o de embeddings
- MemÃ³ria utilizada
- Tamanho dos vetores

---

## ğŸ§  Modelos Suportados

O cÃ³digo usa o modelo `all-MiniLM-L6-v2` para embeddings e estÃ¡ preparado para usar o LLM `mistralai/Mistral-7B-Instruct-v0.1` (ou outro via Transformers) para RAG.

Se nÃ£o quiser usar um modelo pesado, vocÃª pode ajustar o `load_llm()` em `llm.py` para carregar outro mais leve como `tiiuae/falcon-rw-1b`.

---

## âœ… Requisitos

- Python 3.9+
- `sqlite-vec` instalado (ver `requirements.txt`)
- (Opcional) CUDA se for usar um modelo LLM com GPU

---

## ğŸ“„ LicenÃ§a

MIT - sinta-se Ã  vontade para usar e modificar.

---

Desenvolvido com â¤ï¸ usando [LangChain](https://www.langchain.com), [sqlite-vec](https://github.com/sqlite/sqlite-vec), [HuggingFace Transformers](https://huggingface.co/), e [IBGE API](https://servicodados.ibge.gov.br/api/docs/).


---

## ğŸ³ Rodando com Docker

### 1. Build da imagem

Na raiz do projeto:

```
docker build -t ibge-rag .
```

### 2. Executar a indexaÃ§Ã£o

```
docker run --rm -v $(pwd):/app ibge-rag python main.py
```

### 3. Fazer perguntas com RAG

```
docker run -it --rm -v $(pwd):/app ibge-rag python run_rag.py
```

### Dockerfile de exemplo (adicione como `Dockerfile` na raiz):

```
FROM python:3.10-slim

WORKDIR /app
COPY . .

RUN apt-get update && apt-get install -y gcc && \ 
    pip install --no-cache-dir -r requirements.txt

CMD ["python", "main.py"]
```
